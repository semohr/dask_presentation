---
layout: row,dask
section: Python Scaling
---


<Column>

## Numpy and Pandas

```python
import numpy as np
import pandas as pd

# Load data 
data = pd.read_csv('data.csv')

def my_awesome_analysis(data):
    result = np.mean(data["col1"] * data["col2"])
    return result

# Analyse data
results = my_awesome_analysis(data)

# Save results
```

</Column>

<Column>

## Dask 

```python
import dask.array as da
import dask.dataframe as dd

# Load data (multiple files now)
data = dd.read_csv('data/*.csv')

def my_awesome_analysis(data):
    result = da.mean(data["col1"] * data["col2"])
    return result

# Analyse data (notice the .compute() call)
results = my_awesome_analysis(data).compute()

# Save results
```

</Column>

<Note>

Here on the left side we have our example from earlier. We can easliy transform this
already existing code to use Dask. 

The first thing we need to do is to change the
`numpy` and `pandas` imports to `dask.array` and `dask.dataframe` respectively.

This is possible because these dask modules suply the same API as the numpy and pandas
modules. This means that you can use the same functions and methods as you would use with numpy
and pandas but profit from the greater computing and memory handling capabilities of dask.

Here there are some things to keep in mind.

As of now dask most likely will use your local available resources to do the computation. This means
that if you have 4 cores on your machine, dask will create 4 processes and use them to do the
scheduling and computation.

Also notice this line `results = my_awesome_analysis(data).compute()`. This is
because dask is lazy. Dask will not load the data nor will it schedule or run any computation
until you call the `.compute()` method.

Let's take a deeper dive on how dask works in more detail shall we!

[click]

</Note>

<Navbar sections={false} />

